{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Q-Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduction to Q-Learning\n",
    "\n",
    "# Markdown cell explaining the Q-Learning algorithm\n",
    "\"\"\"\n",
    "### What is Q-Learning?\n",
    "\n",
    "Q-Learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for a given finite Markov Decision Process (MDP). It is based on the concept of learning the Q-value, which represents the expected utility of taking a certain action in a given state, and following the optimal policy thereafter.\n",
    "\n",
    "The Q-value is updated iteratively using the following update rule:\n",
    "\n",
    "\\[\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( R_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( Q(s_t, a_t) \\): Current Q-value for state \\( s_t \\) and action \\( a_t \\).\n",
    "- \\( \\alpha \\): Learning rate, which determines how much new information overrides the old information.\n",
    "- \\( R_t \\): Reward received after taking action \\( a_t \\) in state \\( s_t \\).\n",
    "- \\( \\gamma \\): Discount factor, which determines the importance of future rewards.\n",
    "- \\( \\max_{a} Q(s_{t+1}, a) \\): Maximum Q-value for the next state \\( s_{t+1} \\) over all possible actions.\n",
    "\n",
    "The goal of Q-Learning is to iteratively update the Q-values until they converge to the optimal Q-values, which can then be used to derive the optimal policy.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Importing Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for implementing the Q-Learning algorithm:\n",
    "\n",
    "- **NumPy**: A powerful library for numerical computations. It is used for handling arrays, performing mathematical operations, and managing the Q-value table efficiently.\n",
    "- **Matplotlib**: A library for creating visualizations. It will be used to plot the results and visualize the learning process.\n",
    "\n",
    "These libraries are essential for implementing and analyzing the Q-Learning algorithm effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Grid Graph\n",
    "Explain the grid graph structure, where each state is a node, and edges represent possible actions with directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Grid Graph\n",
    "\n",
    "\"\"\"\n",
    "### Defining the Grid Graph\n",
    "\n",
    "The grid graph represents the environment in which the agent operates. Each state in the grid is represented as a node, and the edges between nodes represent possible actions (e.g., moving up, down, left, or right). The graph is defined as a dictionary where:\n",
    "- Keys are the states (nodes) represented as strings.\n",
    "- Values are lists of tuples, where each tuple contains:\n",
    "  - The neighboring state (node) the agent can move to.\n",
    "  - The action (direction) required to move to that neighboring state.\n",
    "\n",
    "This structure allows us to model the environment and define the possible transitions between states.\n",
    "\"\"\"\n",
    "\n",
    "grid_graph = {\n",
    "    \"1\": [(\"2\", \"right\"), (\"6\", \"down\")],\n",
    "    \"2\": [(\"1\", \"left\"), (\"3\", \"right\"), (\"7\", \"down\")],\n",
    "    \"3\": [(\"2\", \"left\"), (\"4\", \"right\"), (\"8\", \"down\")],\n",
    "    \"4\": [(\"3\", \"left\"), (\"5\", \"right\"), (\"9\", \"down\")],\n",
    "    \"5\": [(\"4\", \"left\"), (\"10\", \"down\")],\n",
    "\n",
    "    \"6\": [(\"1\", \"up\"), (\"7\", \"right\"), (\"11\", \"down\")],\n",
    "    \"7\": [(\"2\", \"up\"), (\"6\", \"left\"), (\"8\", \"right\"), (\"12\", \"down\")],\n",
    "    \"8\": [(\"3\", \"up\"), (\"7\", \"left\"), (\"9\", \"right\"), (\"13\", \"down\")],\n",
    "    \"9\": [(\"4\", \"up\"), (\"8\", \"left\"), (\"10\", \"right\"), (\"14\", \"down\")],\n",
    "    \"10\": [(\"5\", \"up\"), (\"9\", \"left\"), (\"15\", \"down\")],\n",
    "\n",
    "    \"11\": [(\"6\", \"up\"), (\"12\", \"right\"), (\"16\", \"down\")],\n",
    "    \"12\": [(\"7\", \"up\"), (\"11\", \"left\"), (\"13\", \"right\"), (\"17\", \"down\")],\n",
    "    \"13\": [(\"8\", \"up\"), (\"12\", \"left\"), (\"14\", \"right\"), (\"18\", \"down\")],\n",
    "    \"14\": [(\"9\", \"up\"), (\"13\", \"left\"), (\"15\", \"right\"), (\"19\", \"down\")],\n",
    "    \"15\": [(\"10\", \"up\"), (\"14\", \"left\"), (\"20\", \"down\")],\n",
    "\n",
    "    \"16\": [(\"11\", \"up\"), (\"17\", \"right\"), (\"21\", \"down\")],\n",
    "    \"17\": [(\"12\", \"up\"), (\"16\", \"left\"), (\"18\", \"right\"), (\"22\", \"down\")],\n",
    "    \"18\": [(\"13\", \"up\"), (\"17\", \"left\"), (\"19\", \"right\"), (\"23\", \"down\")],\n",
    "    \"19\": [(\"14\", \"up\"), (\"18\", \"left\"), (\"20\", \"right\"), (\"24\", \"down\")],\n",
    "    \"20\": [(\"15\", \"up\"), (\"19\", \"left\"), (\"25\", \"down\")],\n",
    "\n",
    "    \"21\": [(\"16\", \"up\"), (\"22\", \"right\")],\n",
    "    \"22\": [(\"17\", \"up\"), (\"21\", \"left\"), (\"23\", \"right\")],\n",
    "    \"23\": [(\"18\", \"up\"), (\"22\", \"left\"), (\"24\", \"right\")],\n",
    "    \"24\": [(\"19\", \"up\"), (\"23\", \"left\"), (\"25\", \"right\")],\n",
    "    \"25\": [(\"20\", \"up\"), (\"24\", \"left\")]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Q-States and Rewards\n",
    "Add a markdown cell explaining the initialization of Q-values for each state-action pair and the reward structure, including penalties and goal rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-States and Rewards\n",
    "\n",
    "\"\"\"\n",
    "### Initializing Q-Values and Rewards\n",
    "\n",
    "In this section, we initialize the Q-values and rewards for each state in the grid environment:\n",
    "\n",
    "- **Q-Values**: Represent the expected utility of taking a specific action in a given state. Initially, all Q-values are set to 0 for every state-action pair.\n",
    "- **Rewards**: Define the immediate reward received upon entering a state. Most states have a default reward of -1 to encourage the agent to find the shortest path to the goal. Special states include:\n",
    "  - **State 0**: A penalty state with a reward of -10.\n",
    "  - **State 25**: The goal state with a reward of +10.\n",
    "\n",
    "This setup ensures that the agent is incentivized to reach the goal state while avoiding unnecessary steps or penalties.\n",
    "\"\"\"\n",
    "\n",
    "# Initialize Q-values for each state-action pair\n",
    "Q_states = {str(i): {\"right\": 0, \"up\": 0, \"left\": 0, \"down\": 0} for i in range(1, 26)}\n",
    "\n",
    "# Define rewards for each state\n",
    "rewards = {str(i): -1 for i in range(1, 26)}\n",
    "rewards[\"0\"] = -10  # Penalty state\n",
    "rewards[\"25\"] = 10  # Goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "Explain the purpose of helper functions for retrieving neighbor states, getting and setting Q-values, and fetching rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "\"\"\"\n",
    "### Helper Functions\n",
    "\n",
    "This section defines helper functions that are essential for the Q-Learning algorithm. These functions include:\n",
    "\n",
    "1. **`get_neighbor_states(state)`**: Retrieves the neighboring states and possible actions for a given state.\n",
    "2. **`get_state_Q(state, action)`**: Fetches the Q-value for a specific state-action pair.\n",
    "3. **`set_state_Q(state, action, new_value)`**: Updates the Q-value for a specific state-action pair.\n",
    "4. **`get_reward(state)`**: Returns the reward associated with a given state.\n",
    "\n",
    "These functions abstract the operations on the grid graph, Q-values, and rewards, making the implementation modular and easier to understand.\n",
    "\"\"\"\n",
    "\n",
    "# Function to retrieve neighboring states and actions\n",
    "def get_neighbor_states(state: str) -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Retrieves the neighboring states and possible actions for a given state.\n",
    "\n",
    "    Args:\n",
    "        state (str): The current state.\n",
    "\n",
    "    Returns:\n",
    "        list[tuple[str, str]]: A list of tuples where each tuple contains:\n",
    "            - The neighboring state.\n",
    "            - The action required to move to that state.\n",
    "    \"\"\"\n",
    "    return grid_graph[state]\n",
    "\n",
    "# Function to get the Q-value for a specific state-action pair\n",
    "def get_state_Q(state: str, action: str) -> int:\n",
    "    \"\"\"\n",
    "    Fetches the Q-value for a specific state-action pair.\n",
    "\n",
    "    Args:\n",
    "        state (str): The current state.\n",
    "        action (str): The action to be taken.\n",
    "\n",
    "    Returns:\n",
    "        int: The Q-value for the given state-action pair.\n",
    "    \"\"\"\n",
    "    return Q_states[state][action]\n",
    "\n",
    "# Function to set the Q-value for a specific state-action pair\n",
    "def set_state_Q(state: str, action: str, new_value: float) -> None:\n",
    "    \"\"\"\n",
    "    Updates the Q-value for a specific state-action pair.\n",
    "\n",
    "    Args:\n",
    "        state (str): The current state.\n",
    "        action (str): The action to be taken.\n",
    "        new_value (float): The new Q-value to be set.\n",
    "    \"\"\"\n",
    "    Q_states[state][action] = new_value\n",
    "\n",
    "# Function to get the reward for a specific state\n",
    "def get_reward(state: str) -> int:\n",
    "    \"\"\"\n",
    "    Returns the reward associated with a given state.\n",
    "\n",
    "    Args:\n",
    "        state (str): The current state.\n",
    "\n",
    "    Returns:\n",
    "        int: The reward for the given state.\n",
    "    \"\"\"\n",
    "    return rewards[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Value Update Function\n",
    "Add a markdown cell explaining the Q-value update function, including the role of learning rate, discount factor, and step penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Value Update Function\n",
    "\n",
    "The Q-value update function is a critical component of the Q-Learning algorithm. It updates the Q-value for a given state-action pair based on the reward received and the maximum Q-value of the next state. The update rule is as follows:\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left( R_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \\right)\n",
    "$$\n",
    "\n",
    "#### Components:\n",
    "- **Learning Rate ($ \\alpha $)**: Determines how much the new information overrides the old information. A higher value means the agent learns faster but may overshoot the optimal value.\n",
    "- **Discount Factor ($ \\gamma $)**: Determines the importance of future rewards. A value close to 1 emphasizes long-term rewards, while a value close to 0 focuses on immediate rewards.\n",
    "- **Step Penalty**: An additional penalty applied to discourage unnecessary steps, encouraging the agent to find the shortest path to the goal.\n",
    "\n",
    "This function ensures that the agent learns the optimal policy by iteratively updating the Q-values based on the observed rewards and transitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Value Update Function\n",
    "\n",
    "\n",
    "def get_Q_update(\n",
    "    state_Q: float,\n",
    "    state_reward: float,\n",
    "    neighbor_states: list[str],\n",
    "    learning_rate: float = 0.5,\n",
    "    discount_factor: float = 0.5,\n",
    "    step_penalty: float = 0\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Updates the Q-value for a given state-action pair.\n",
    "\n",
    "    Args:\n",
    "        state_Q (float): Current Q-value of the state-action pair.\n",
    "        state_reward (float): Reward received for the current state.\n",
    "        neighbor_states (list[str]): List of neighboring states and actions.\n",
    "        learning_rate (float, optional): Learning rate (alpha). Defaults to 0.5.\n",
    "        discount_factor (float, optional): Discount factor (gamma). Defaults to 0.5.\n",
    "        step_penalty (float, optional): Penalty for taking a step. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        float: Updated Q-value for the state-action pair.\n",
    "    \"\"\"\n",
    "    # Calculate the maximum Q-value among the neighboring states\n",
    "    neighbor_states_Q = map(lambda x: get_state_Q(*x), neighbor_states)\n",
    "    \n",
    "    # Apply the Q-value update formula\n",
    "    return state_Q + learning_rate * (\n",
    "        (state_reward + step_penalty) + discount_factor * max(neighbor_states_Q) - state_Q\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighbor Q-Values and Next State Selection\n",
    "Explain the logic for calculating neighbor Q-values and selecting the next state based on the highest Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Neighbor Q-Values\n",
    "\n",
    "In this section, we calculate the Q-values of all neighboring states for a given current state. This helps the agent evaluate the potential outcomes of each possible action. The function `get_neighbor_states_Q` retrieves the Q-values for all neighboring states and their corresponding actions.\n",
    "\n",
    "#### Formula:\n",
    "For each neighboring state \\( s' \\) and action \\( a \\), the Q-value is given by:\n",
    "\\[\n",
    "Q(s, a) = Q_{\\text{states}}[s][a]\n",
    "\\]\n",
    "\n",
    "This step is crucial for determining the best action to take from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neighbor Q-Values \n",
    "\n",
    "def get_neighbor_states_Q(current_state: str, neighbor_states: list[tuple[str, str]]) -> list[float]:\n",
    "    \"\"\"\n",
    "    Retrieves the Q-values for all neighboring states and actions.\n",
    "\n",
    "    Args:\n",
    "        current_state (str): The current state.\n",
    "        neighbor_states (list[tuple[str, str]]): List of neighboring states and actions.\n",
    "\n",
    "    Returns:\n",
    "        list[float]: List of Q-values for the neighboring states and actions.\n",
    "    \"\"\"\n",
    "    return list(map(lambda x: get_state_Q(current_state, x[1]), neighbor_states))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the Next State\n",
    "\n",
    "After calculating the Q-values of all neighboring states, the agent selects the next state based on the action with the highest Q-value. The function `get_next_state` identifies the best action and its corresponding state.\n",
    "\n",
    "#### Logic:\n",
    "1. Compute the Q-values for all neighboring states.\n",
    "2. Identify the action with the maximum Q-value.\n",
    "3. Return the corresponding neighboring state and action.\n",
    "\n",
    "This ensures that the agent follows a greedy policy to maximize its expected reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next State Selection\n",
    "\n",
    "def get_next_state(current_state: str, neighbor_states: list[tuple[str, str]]) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Selects the next state based on the highest Q-value among neighboring states.\n",
    "\n",
    "    Args:\n",
    "        current_state (str): The current state.\n",
    "        neighbor_states (list[tuple[str, str]]): List of neighboring states and actions.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, str]: The next state and the action leading to it.\n",
    "    \"\"\"\n",
    "    neighbor_states_Q = get_neighbor_states_Q(current_state, neighbor_states)\n",
    "    max_index = np.argmax(neighbor_states_Q)\n",
    "    return neighbor_states[max_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Q-Learning Loop\n",
    "\n",
    "\n",
    "### Main Q-Learning Loop\n",
    "\n",
    "This section implements the main loop of the Q-Learning algorithm. The loop iteratively updates the Q-values, selects actions, and transitions between states until the goal state is reached or a maximum number of iterations is exceeded.\n",
    "\n",
    "#### Steps:\n",
    "1. **Initialization**: Start from an initial state (e.g., state \"1\").\n",
    "2. **Neighbor States**: Retrieve the neighboring states and their Q-values.\n",
    "3. **Action Selection**: Choose the next action based on the highest Q-value (greedy policy).\n",
    "4. **Q-Value Update**: Update the Q-value of the current state-action pair using the Q-Learning update rule.\n",
    "5. **Transition**: Move to the next state and repeat the process.\n",
    "6. **Stopping Condition**: Stop when the goal state is reached or the maximum number of iterations is reached.\n",
    "\n",
    "This loop ensures that the agent learns the optimal policy by exploring the environment and updating its Q-values based on observed rewards and transitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Q-Learning Loop\n",
    "\n",
    "current_state = \"1\"  # Initialize the starting state\n",
    "states_follow_up = [current_state]  # Track the sequence of states visited\n",
    "\n",
    "# Maximum number of iterations to prevent infinite loops\n",
    "max_iterations = 1_000_000\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    print(f\"Step {i + 1}:\")\n",
    "    print(f\"\\tCurrent state: {current_state}\")\n",
    "\n",
    "    # Check if the goal state is reached\n",
    "    if current_state == \"25\":\n",
    "        print(f\"Goal state found at iteration {i + 1}\")\n",
    "        break\n",
    "\n",
    "    # Retrieve neighboring states and their Q-values\n",
    "    neighbor_states = get_neighbor_states(state=current_state)\n",
    "    neighbor_states_Q = get_neighbor_states_Q(current_state=current_state, neighbor_states=neighbor_states)\n",
    "    print(f\"\\tNeighbor states: {neighbor_states}\")\n",
    "    print(f\"\\tNeighbor states Q-values: {neighbor_states_Q}\")\n",
    "\n",
    "    # Select the next state based on the highest Q-value\n",
    "    next_state = get_next_state(current_state=current_state, neighbor_states=neighbor_states)\n",
    "    next_state_Q = get_state_Q(state=current_state, action=next_state[1])\n",
    "    next_state_reward = get_reward(next_state[0])\n",
    "    print(f'\\tNext action: \"{next_state[1]}\" to state {next_state[0]} with Q-value = {next_state_Q} and reward = {next_state_reward}')\n",
    "\n",
    "    # Update the Q-value for the current state-action pair\n",
    "    updated_Q = get_Q_update(\n",
    "        state_Q=next_state_Q,\n",
    "        state_reward=next_state_reward,\n",
    "        neighbor_states=neighbor_states,\n",
    "        learning_rate=0.3,\n",
    "        discount_factor=0.9,\n",
    "        step_penalty=-0.5\n",
    "    )\n",
    "    set_state_Q(state=current_state, action=next_state[1], new_value=updated_Q)\n",
    "    print(f\"\\tUpdated Q-value: {get_state_Q(state=current_state, action=next_state[1])}\")\n",
    "\n",
    "    # Transition to the next state\n",
    "    current_state = next_state[0]\n",
    "    states_follow_up.append(current_state)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
