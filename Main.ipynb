{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha  [R_t + \\gamma \\max_{a} Q(s_{t+1,},a) - Q(s_t, a_t)]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid 4x4\n",
    "grid_graph = {\n",
    "    \"1\": [(\"2\", \"right\"), (\"5\", \"down\")],\n",
    "    \"2\": [(\"1\", \"left\"), (\"3\", \"right\"), (\"6\", \"down\")],\n",
    "    \"3\": [(\"2\", \"left\"), (\"4\", \"right\"), (\"7\", \"down\")],\n",
    "    \"4\": [(\"3\", \"left\"), (\"8\", \"down\")],\n",
    "    \"5\": [(\"1\", \"up\"), (\"6\", \"right\"), (\"9\", \"down\")],\n",
    "    \"6\": [(\"2\", \"up\"), (\"5\", \"left\"), (\"7\", \"right\"), (\"10\", \"down\")],\n",
    "    \"7\": [(\"3\", \"up\"), (\"6\", \"left\"), (\"8\", \"right\"), (\"11\", \"down\")],\n",
    "    \"8\": [(\"4\", \"up\"), (\"7\", \"left\"), (\"12\", \"down\")],\n",
    "    \"9\": [(\"5\", \"up\"), (\"10\", \"right\"), (\"13\", \"down\")],\n",
    "    \"10\": [(\"6\", \"up\"), (\"9\", \"left\"), (\"11\", \"right\"), (\"14\", \"down\")],\n",
    "    \"11\": [(\"7\", \"up\"), (\"10\", \"left\"), (\"12\", \"right\"), (\"15\", \"down\")],\n",
    "    \"12\": [(\"8\", \"up\"), (\"11\", \"left\"), (\"16\", \"down\")],\n",
    "    \"13\": [(\"9\", \"up\"), (\"14\", \"right\")],\n",
    "    \"14\": [(\"10\", \"up\"), (\"13\", \"left\"), (\"15\", \"right\")],\n",
    "    \"15\": [(\"11\", \"up\"), (\"14\", \"left\"), (\"16\", \"right\")],\n",
    "    \"16\": [(\"12\", \"up\"), (\"15\", \"left\")],\n",
    "}\n",
    "\n",
    "# Grid 3x3\n",
    "# grid_graph = {\n",
    "#     \"1\": [(\"2\", \"right\"), (\"4\", \"down\")],\n",
    "#     \"2\": [(\"1\", \"left\"), (\"3\", \"right\"), (\"5\", \"down\")],\n",
    "#     \"3\": [(\"2\", \"left\"), (\"6\", \"down\")],\n",
    "\n",
    "#     \"4\": [(\"1\", \"up\"), (\"5\", \"right\"), (\"7\", \"down\")],\n",
    "#     \"5\": [(\"2\", \"up\"), (\"4\", \"left\"), (\"6\", \"right\"), (\"8\", \"down\")],\n",
    "#     \"6\": [(\"3\", \"up\"), (\"5\", \"left\"), (\"9\", \"down\")],\n",
    "\n",
    "#     \"7\": [(\"4\", \"up\"), (\"8\", \"right\")],\n",
    "#     \"8\": [(\"5\", \"up\"), (\"7\", \"left\"), (\"9\", \"right\")],\n",
    "#     \"9\": [(\"6\", \"up\"), (\"8\", \"left\")]\n",
    "# }\n",
    "\n",
    "# Grid 5x5\n",
    "# grid_graph = {\n",
    "#     \"1\": [(\"2\", \"right\"), (\"6\", \"down\")],\n",
    "#     \"2\": [(\"1\", \"left\"), (\"3\", \"right\"), (\"7\", \"down\")],\n",
    "#     \"3\": [(\"2\", \"left\"), (\"4\", \"right\"), (\"8\", \"down\")],\n",
    "#     \"4\": [(\"3\", \"left\"), (\"5\", \"right\"), (\"9\", \"down\")],\n",
    "#     \"5\": [(\"4\", \"left\"), (\"10\", \"down\")],\n",
    "\n",
    "#     \"6\": [(\"1\", \"up\"), (\"7\", \"right\"), (\"11\", \"down\")],\n",
    "#     \"7\": [(\"2\", \"up\"), (\"6\", \"left\"), (\"8\", \"right\"), (\"12\", \"down\")],\n",
    "#     \"8\": [(\"3\", \"up\"), (\"7\", \"left\"), (\"9\", \"right\"), (\"13\", \"down\")],\n",
    "#     \"9\": [(\"4\", \"up\"), (\"8\", \"left\"), (\"10\", \"right\"), (\"14\", \"down\")],\n",
    "#     \"10\": [(\"5\", \"up\"), (\"9\", \"left\"), (\"15\", \"down\")],\n",
    "\n",
    "#     \"11\": [(\"6\", \"up\"), (\"12\", \"right\"), (\"16\", \"down\")],\n",
    "#     \"12\": [(\"7\", \"up\"), (\"11\", \"left\"), (\"13\", \"right\"), (\"17\", \"down\")],\n",
    "#     \"13\": [(\"8\", \"up\"), (\"12\", \"left\"), (\"14\", \"right\"), (\"18\", \"down\")],\n",
    "#     \"14\": [(\"9\", \"up\"), (\"13\", \"left\"), (\"15\", \"right\"), (\"19\", \"down\")],\n",
    "#     \"15\": [(\"10\", \"up\"), (\"14\", \"left\"), (\"20\", \"down\")],\n",
    "\n",
    "#     \"16\": [(\"11\", \"up\"), (\"17\", \"right\"), (\"21\", \"down\")],\n",
    "#     \"17\": [(\"12\", \"up\"), (\"16\", \"left\"), (\"18\", \"right\"), (\"22\", \"down\")],\n",
    "#     \"18\": [(\"13\", \"up\"), (\"17\", \"left\"), (\"19\", \"right\"), (\"23\", \"down\")],\n",
    "#     \"19\": [(\"14\", \"up\"), (\"18\", \"left\"), (\"20\", \"right\"), (\"24\", \"down\")],\n",
    "#     \"20\": [(\"15\", \"up\"), (\"19\", \"left\"), (\"25\", \"down\")],\n",
    "\n",
    "#     \"21\": [(\"16\", \"up\"), (\"22\", \"right\")],\n",
    "#     \"22\": [(\"17\", \"up\"), (\"21\", \"left\"), (\"23\", \"right\")],\n",
    "#     \"23\": [(\"18\", \"up\"), (\"22\", \"left\"), (\"24\", \"right\")],\n",
    "#     \"24\": [(\"19\", \"up\"), (\"23\", \"left\"), (\"25\", \"right\")],\n",
    "#     \"25\": [(\"20\", \"up\"), (\"24\", \"left\")]\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_states = {str(i): {\"right\": 0, \"up\": 0, \"left\": 0, \"down\": 0} for i in range(1, 26)}\n",
    "\n",
    "\n",
    "rewards = {str(i): -1 for i in range(1, len(grid_graph) + 1)}\n",
    "rewards[\"1\"] = -1\n",
    "rewards[\"16\"] = 100\n",
    "\n",
    "\n",
    "def get_neighbor_states(state: str) -> list[tuple[str, str]]:\n",
    "    return grid_graph[state]\n",
    "\n",
    "\n",
    "def get_state_Q(state: str, action: str) -> int:\n",
    "    return Q_states[state][action]\n",
    "\n",
    "\n",
    "def set_state_Q(state: str, action: str, new_value: float) -> int:\n",
    "    Q_states[state][action] = new_value\n",
    "\n",
    "\n",
    "def get_reward(state: str, goal_state: str, distance_reward: bool = False) -> int:\n",
    "    if distance_reward:\n",
    "        relative_distance = abs(int(state) - int(goal_state))\n",
    "        relative_distance_reward = -relative_distance\n",
    "        return rewards[state] + relative_distance_reward\n",
    "    return rewards[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Q_update(\n",
    "    state_Q: float,\n",
    "    state_reward: float,\n",
    "    neighbor_states: list[str],\n",
    "    learning_rate: float = 0.5,\n",
    "    discount_factor: float = 0.5,\n",
    "    step_penalty:float = 0\n",
    ") -> float:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        state_Q (float): Q value of a state\n",
    "        state_reward (float): reward of a state\n",
    "        learning_rate (float, optional): how much to update the Q-value. Defaults to .5.\n",
    "        discount_factor (float, optional): how much future rewards are taken into account. Defaults to .5.\n",
    "\n",
    "    Returns:\n",
    "        float: _description_\n",
    "    \"\"\"\n",
    "    neighbor_states_Q = map(lambda x: get_state_Q(*x), neighbor_states)\n",
    "    return state_Q + learning_rate * (\n",
    "        (state_reward + step_penalty) + discount_factor * max(neighbor_states_Q) - state_Q\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbor_states_Q(current_state, neighbor_states):\n",
    "    return list(\n",
    "        map(lambda x: get_state_Q(*x), [[current_state, j] for _, j in neighbor_states])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(current_state, neighbor_states):\n",
    "    neighbor_states_Q = get_neighbor_states_Q(current_state, neighbor_states)\n",
    "    return neighbor_states[np.argmax(neighbor_states_Q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(current_state, neighbor_states, epsilon=0):\n",
    "    if np.random.random() < epsilon:\n",
    "        print(\"Random action\")\n",
    "        np.random.shuffle(neighbor_states)\n",
    "        return  neighbor_states[0]\n",
    "    return get_next_state(current_state, neighbor_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "\tcurrent state: 1\n",
      "\tneighbor states: [('2', 'right'), ('5', 'down')] \n",
      "\tneighbor states Q values: [0, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 2 with Q value = 0 and reward = -15\n",
      "\tUpdates Q value: -11.899999999999999\n",
      "Step 2:\n",
      "\tcurrent state: 2\n",
      "\tneighbor states: [('1', 'left'), ('3', 'right'), ('6', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "\tNext action: \"left\" to state 1 with Q value = 0 and reward = -16\n",
      "\tUpdates Q value: -12.6\n",
      "Step 3:\n",
      "\tcurrent state: 1\n",
      "\tneighbor states: [('2', 'right'), ('5', 'down')] \n",
      "\tneighbor states Q values: [-11.899999999999999, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 2 with Q value = -11.899999999999999 and reward = -15\n",
      "\tUpdates Q value: -15.469999999999999\n",
      "Step 4:\n",
      "\tcurrent state: 2\n",
      "\tneighbor states: [('1', 'left'), ('3', 'right'), ('6', 'down')] \n",
      "\tneighbor states Q values: [-12.6, 0, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 3 with Q value = 0 and reward = -14\n",
      "\tUpdates Q value: -11.2\n",
      "Step 5:\n",
      "\tcurrent state: 3\n",
      "\tneighbor states: [('2', 'left'), ('4', 'right'), ('7', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 4 with Q value = 0 and reward = -13\n",
      "\tUpdates Q value: -10.5\n",
      "Step 6:\n",
      "\tcurrent state: 4\n",
      "\tneighbor states: [('3', 'left'), ('8', 'down')] \n",
      "\tneighbor states Q values: [0, 0] \n",
      "Random action\n",
      "\tNext action: \"left\" to state 3 with Q value = 0 and reward = -14\n",
      "\tUpdates Q value: -11.2\n",
      "Step 7:\n",
      "\tcurrent state: 3\n",
      "\tneighbor states: [('4', 'right'), ('7', 'down'), ('2', 'left')] \n",
      "\tneighbor states Q values: [-10.5, 0, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 4 with Q value = -10.5 and reward = -13\n",
      "\tUpdates Q value: -13.65\n",
      "Step 8:\n",
      "\tcurrent state: 4\n",
      "\tneighbor states: [('3', 'left'), ('8', 'down')] \n",
      "\tneighbor states Q values: [-11.2, 0] \n",
      "\tNext action: \"down\" to state 8 with Q value = 0 and reward = -9\n",
      "\tUpdates Q value: -7.699999999999999\n",
      "Step 9:\n",
      "\tcurrent state: 8\n",
      "\tneighbor states: [('4', 'up'), ('7', 'left'), ('12', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "\tNext action: \"up\" to state 4 with Q value = 0 and reward = -13\n",
      "\tUpdates Q value: -10.5\n",
      "Step 10:\n",
      "\tcurrent state: 4\n",
      "\tneighbor states: [('3', 'left'), ('8', 'down')] \n",
      "\tneighbor states Q values: [-11.2, -7.699999999999999] \n",
      "\tNext action: \"down\" to state 8 with Q value = -7.699999999999999 and reward = -9\n",
      "\tUpdates Q value: -10.01\n",
      "Step 11:\n",
      "\tcurrent state: 8\n",
      "\tneighbor states: [('4', 'up'), ('7', 'left'), ('12', 'down')] \n",
      "\tneighbor states Q values: [-10.5, 0, 0] \n",
      "\tNext action: \"left\" to state 7 with Q value = 0 and reward = -10\n",
      "\tUpdates Q value: -8.399999999999999\n",
      "Step 12:\n",
      "\tcurrent state: 7\n",
      "\tneighbor states: [('3', 'up'), ('6', 'left'), ('8', 'right'), ('11', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0, 0] \n",
      "\tNext action: \"up\" to state 3 with Q value = 0 and reward = -14\n",
      "\tUpdates Q value: -11.2\n",
      "Step 13:\n",
      "\tcurrent state: 3\n",
      "\tneighbor states: [('4', 'right'), ('2', 'left'), ('7', 'down')] \n",
      "\tneighbor states Q values: [-13.65, 0, 0] \n",
      "\tNext action: \"left\" to state 2 with Q value = 0 and reward = -15\n",
      "\tUpdates Q value: -11.899999999999999\n",
      "Step 14:\n",
      "\tcurrent state: 2\n",
      "\tneighbor states: [('3', 'right'), ('1', 'left'), ('6', 'down')] \n",
      "\tneighbor states Q values: [-11.2, -12.6, 0] \n",
      "\tNext action: \"down\" to state 6 with Q value = 0 and reward = -11\n",
      "\tUpdates Q value: -9.1\n",
      "Step 15:\n",
      "\tcurrent state: 6\n",
      "\tneighbor states: [('2', 'up'), ('5', 'left'), ('7', 'right'), ('10', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0, 0] \n",
      "\tNext action: \"up\" to state 2 with Q value = 0 and reward = -15\n",
      "\tUpdates Q value: -11.899999999999999\n",
      "Step 16:\n",
      "\tcurrent state: 2\n",
      "\tneighbor states: [('3', 'right'), ('1', 'left'), ('6', 'down')] \n",
      "\tneighbor states Q values: [-11.2, -12.6, -9.1] \n",
      "\tNext action: \"down\" to state 6 with Q value = -9.1 and reward = -11\n",
      "\tUpdates Q value: -11.83\n",
      "Step 17:\n",
      "\tcurrent state: 6\n",
      "\tneighbor states: [('2', 'up'), ('5', 'left'), ('7', 'right'), ('10', 'down')] \n",
      "\tneighbor states Q values: [-11.899999999999999, 0, 0, 0] \n",
      "\tNext action: \"left\" to state 5 with Q value = 0 and reward = -12\n",
      "\tUpdates Q value: -9.799999999999999\n",
      "Step 18:\n",
      "\tcurrent state: 5\n",
      "\tneighbor states: [('1', 'up'), ('6', 'right'), ('9', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "\tNext action: \"up\" to state 1 with Q value = 0 and reward = -16\n",
      "\tUpdates Q value: -12.6\n",
      "Step 19:\n",
      "\tcurrent state: 1\n",
      "\tneighbor states: [('2', 'right'), ('5', 'down')] \n",
      "\tneighbor states Q values: [-15.469999999999999, 0] \n",
      "\tNext action: \"down\" to state 5 with Q value = 0 and reward = -12\n",
      "\tUpdates Q value: -9.799999999999999\n",
      "Step 20:\n",
      "\tcurrent state: 5\n",
      "\tneighbor states: [('1', 'up'), ('6', 'right'), ('9', 'down')] \n",
      "\tneighbor states Q values: [-12.6, 0, 0] \n",
      "\tNext action: \"right\" to state 6 with Q value = 0 and reward = -11\n",
      "\tUpdates Q value: -9.1\n",
      "Step 21:\n",
      "\tcurrent state: 6\n",
      "\tneighbor states: [('2', 'up'), ('5', 'left'), ('7', 'right'), ('10', 'down')] \n",
      "\tneighbor states Q values: [-11.899999999999999, -9.799999999999999, 0, 0] \n",
      "\tNext action: \"right\" to state 7 with Q value = 0 and reward = -10\n",
      "\tUpdates Q value: -8.399999999999999\n",
      "Step 22:\n",
      "\tcurrent state: 7\n",
      "\tneighbor states: [('3', 'up'), ('6', 'left'), ('8', 'right'), ('11', 'down')] \n",
      "\tneighbor states Q values: [-11.2, 0, 0, 0] \n",
      "\tNext action: \"left\" to state 6 with Q value = 0 and reward = -11\n",
      "\tUpdates Q value: -9.1\n",
      "Step 23:\n",
      "\tcurrent state: 6\n",
      "\tneighbor states: [('2', 'up'), ('5', 'left'), ('7', 'right'), ('10', 'down')] \n",
      "\tneighbor states Q values: [-11.899999999999999, -9.799999999999999, -8.399999999999999, 0] \n",
      "\tNext action: \"down\" to state 10 with Q value = 0 and reward = -7\n",
      "\tUpdates Q value: -6.3\n",
      "Step 24:\n",
      "\tcurrent state: 10\n",
      "\tneighbor states: [('6', 'up'), ('9', 'left'), ('11', 'right'), ('14', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0, 0] \n",
      "\tNext action: \"up\" to state 6 with Q value = 0 and reward = -11\n",
      "\tUpdates Q value: -9.1\n",
      "Step 25:\n",
      "\tcurrent state: 6\n",
      "\tneighbor states: [('2', 'up'), ('5', 'left'), ('7', 'right'), ('10', 'down')] \n",
      "\tneighbor states Q values: [-11.899999999999999, -9.799999999999999, -8.399999999999999, -6.3] \n",
      "\tNext action: \"down\" to state 10 with Q value = -6.3 and reward = -7\n",
      "\tUpdates Q value: -8.19\n",
      "Step 26:\n",
      "\tcurrent state: 10\n",
      "\tneighbor states: [('6', 'up'), ('9', 'left'), ('11', 'right'), ('14', 'down')] \n",
      "\tneighbor states Q values: [-9.1, 0, 0, 0] \n",
      "\tNext action: \"left\" to state 9 with Q value = 0 and reward = -8\n",
      "\tUpdates Q value: -7.0\n",
      "Step 27:\n",
      "\tcurrent state: 9\n",
      "\tneighbor states: [('5', 'up'), ('10', 'right'), ('13', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "\tNext action: \"up\" to state 5 with Q value = 0 and reward = -12\n",
      "\tUpdates Q value: -9.799999999999999\n",
      "Step 28:\n",
      "\tcurrent state: 5\n",
      "\tneighbor states: [('1', 'up'), ('6', 'right'), ('9', 'down')] \n",
      "\tneighbor states Q values: [-12.6, -9.1, 0] \n",
      "\tNext action: \"down\" to state 9 with Q value = 0 and reward = -8\n",
      "\tUpdates Q value: -7.0\n",
      "Step 29:\n",
      "\tcurrent state: 9\n",
      "\tneighbor states: [('5', 'up'), ('10', 'right'), ('13', 'down')] \n",
      "\tneighbor states Q values: [-9.799999999999999, 0, 0] \n",
      "\tNext action: \"right\" to state 10 with Q value = 0 and reward = -7\n",
      "\tUpdates Q value: -6.3\n",
      "Step 30:\n",
      "\tcurrent state: 10\n",
      "\tneighbor states: [('6', 'up'), ('9', 'left'), ('11', 'right'), ('14', 'down')] \n",
      "\tneighbor states Q values: [-9.1, -7.0, 0, 0] \n",
      "\tNext action: \"right\" to state 11 with Q value = 0 and reward = -6\n",
      "\tUpdates Q value: -5.6\n",
      "Step 31:\n",
      "\tcurrent state: 11\n",
      "\tneighbor states: [('7', 'up'), ('10', 'left'), ('12', 'right'), ('15', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0, 0] \n",
      "\tNext action: \"up\" to state 7 with Q value = 0 and reward = -10\n",
      "\tUpdates Q value: -8.399999999999999\n",
      "Step 32:\n",
      "\tcurrent state: 7\n",
      "\tneighbor states: [('3', 'up'), ('6', 'left'), ('8', 'right'), ('11', 'down')] \n",
      "\tneighbor states Q values: [-11.2, -9.1, 0, 0] \n",
      "\tNext action: \"right\" to state 8 with Q value = 0 and reward = -9\n",
      "\tUpdates Q value: -7.699999999999999\n",
      "Step 33:\n",
      "\tcurrent state: 8\n",
      "\tneighbor states: [('4', 'up'), ('7', 'left'), ('12', 'down')] \n",
      "\tneighbor states Q values: [-10.5, -8.399999999999999, 0] \n",
      "\tNext action: \"down\" to state 12 with Q value = 0 and reward = -5\n",
      "\tUpdates Q value: -4.8999999999999995\n",
      "Step 34:\n",
      "\tcurrent state: 12\n",
      "\tneighbor states: [('8', 'up'), ('11', 'left'), ('16', 'down')] \n",
      "\tneighbor states Q values: [0, 0, 0] \n",
      "\tNext action: \"up\" to state 8 with Q value = 0 and reward = -9\n",
      "\tUpdates Q value: -7.699999999999999\n",
      "Step 35:\n",
      "\tcurrent state: 8\n",
      "\tneighbor states: [('4', 'up'), ('7', 'left'), ('12', 'down')] \n",
      "\tneighbor states Q values: [-10.5, -8.399999999999999, -4.8999999999999995] \n",
      "\tNext action: \"down\" to state 12 with Q value = -4.8999999999999995 and reward = -5\n",
      "\tUpdates Q value: -6.369999999999999\n",
      "Step 36:\n",
      "\tcurrent state: 12\n",
      "\tneighbor states: [('8', 'up'), ('11', 'left'), ('16', 'down')] \n",
      "\tneighbor states Q values: [-7.699999999999999, 0, 0] \n",
      "\tNext action: \"left\" to state 11 with Q value = 0 and reward = -6\n",
      "\tUpdates Q value: -5.6\n",
      "Step 37:\n",
      "\tcurrent state: 11\n",
      "\tneighbor states: [('7', 'up'), ('10', 'left'), ('12', 'right'), ('15', 'down')] \n",
      "\tneighbor states Q values: [-8.399999999999999, 0, 0, 0] \n",
      "Random action\n",
      "\tNext action: \"right\" to state 12 with Q value = 0 and reward = -5\n",
      "\tUpdates Q value: -4.8999999999999995\n",
      "Step 38:\n",
      "\tcurrent state: 12\n",
      "\tneighbor states: [('8', 'up'), ('11', 'left'), ('16', 'down')] \n",
      "\tneighbor states Q values: [-7.699999999999999, -5.6, 0] \n",
      "\tNext action: \"down\" to state 16 with Q value = 0 and reward = 100\n",
      "\tUpdates Q value: 68.6\n",
      "Step 39:\n",
      "\tcurrent state: 16\n",
      "Goal state found at 39 iteration\n"
     ]
    }
   ],
   "source": [
    "current_state = \"1\"\n",
    "goal_state = \"16\"\n",
    "states_follow_up = [current_state]\n",
    "\n",
    "epsilon = .9\n",
    "\n",
    "for i in range(1_000):\n",
    "    print(f\"Step {i+1}:\")\n",
    "    print(f\"\\tcurrent state: {current_state}\")\n",
    "\n",
    "    if current_state == \"16\":\n",
    "        print(f\"Goal state found at {i+1} iteration\")\n",
    "        break\n",
    "\n",
    "    neighbor_states = get_neighbor_states(state=current_state)\n",
    "    neighbor_states_Q = get_neighbor_states_Q(\n",
    "        current_state=current_state, neighbor_states=neighbor_states\n",
    "    )\n",
    "\n",
    "    print(f\"\\tneighbor states: {neighbor_states} \")\n",
    "    print(f\"\\tneighbor states Q values: {neighbor_states_Q} \")\n",
    "\n",
    "    \n",
    "    next_state = choose_action(current_state=current_state, neighbor_states=neighbor_states, epsilon=epsilon)\n",
    "    \n",
    "    if epsilon > 0.01:\n",
    "        epsilon = epsilon * .8\n",
    "    \n",
    "    next_action_state_Q = get_state_Q(state=current_state, action=next_state[1])\n",
    "    next_state_reward = get_reward(next_state[0], goal_state, distance_reward=True)\n",
    "    print(\n",
    "        f'\\tNext action: \"{next_state[1]}\" to state {next_state[0]} with Q value = {next_action_state_Q} and reward = {next_state_reward}'\n",
    "    )\n",
    "\n",
    "    updated_Q = get_Q_update(\n",
    "        state_Q=next_action_state_Q,\n",
    "        state_reward=next_state_reward,\n",
    "        neighbor_states=neighbor_states,\n",
    "        learning_rate=0.7,\n",
    "        discount_factor=0.9,\n",
    "        step_penalty=-2,\n",
    "    )\n",
    "\n",
    "    set_state_Q(state=current_state, action=next_state[1], new_value=updated_Q)\n",
    "    print(\n",
    "        f\"\\tUpdates Q value: {get_state_Q(state=current_state, action=next_state[1])}\"\n",
    "    )\n",
    "\n",
    "    current_state = next_state[0]\n",
    "    states_follow_up.append(current_state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
